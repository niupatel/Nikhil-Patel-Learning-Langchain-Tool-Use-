Core components Streaming
Stream real-time updates from agent runs
LangChain implements a streaming system to surface real-time updates.
Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By
displaying output progressively, even before a complete response is ready, streaming
significantly improves user experience �UX�, particularly when dealing with the latency of
LLMs.
LangChainʼs streaming system lets you surface live feedback from agent runs to your
application.
Whatʼs possible with LangChain streaming:
See the  section below for additional end-to-end examples.
 — get state updates after each agent step.
 — stream language model tokens as theyʼre generated.
 — emit user-defined signals (e.g., 
).
 — choose from  (agent progress), 
�LLM tokens + metadata), or  (arbitrary user data).
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
1 of 18 1/20/2026, 3:00 PM
Pass one or more of the following stream modes as a list to the  method:
Streams state updates after each agent step. If multiple updates are made in
the same step (e.g., multiple nodes are run), those updates are streamed
separately.
Streams tuples of  from any graph nodes where an LLM is
invoked.
Streams custom data from inside your graph nodes using the stream writer.
To stream agent progress, use the  method with . This
emits an event after every agent step.
For example, if you have an agent that calls a tool once, you should see the following
updates:
:  with tool call requests
:  with execution result
: Final AI response
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
2 of 18 1/20/2026, 3:00 PM
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
3 of 18 1/20/2026, 3:00 PM
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
4 of 18 1/20/2026, 3:00 PM
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
5 of 18 1/20/2026, 3:00 PM
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
6 of 18 1/20/2026, 3:00 PM
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
7 of 18 1/20/2026, 3:00 PM
import z from "zod";
import { createAgent, tool } from "langchain";
const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
8 of 18 1/20/2026, 3:00 PM
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);
const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getWeather],
});
for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "updates" }
)) {
    const [step, content] = Object.entries(chunk)[0];
    console.log(`step: ${step}`);
    console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         // ...
 *         "tool_calls": [
 *           {
 *             "name": "get_weather",
 *             "args": {
 *               "city": "San Francisco"
 *             },
 *             "type": "tool_call",
 *             "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"
 *           }
 *         ],
 *         // ...
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
9 of 18 1/20/2026, 3:00 PM
To stream tokens as they are produced by the LLM, use :
 *       }
 *     }
 *   ]
 * }
 * step: tools
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The weather in San Francisco is always sunny!",
 *         "name": "get_weather",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The latest update says: The weather in San Francisco is always sunny!\n\n
 *         // ...
 *       }
 *     }
 *   ]
 * }
 */
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
10 of 18 1/20/2026, 3:00 PM
To stream updates from tools as they are executed, you can use the  parameter
from the configuration.
import z from "zod";
import { createAgent, tool } from "langchain";
const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);
const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});
for await (const [token, metadata] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "messages" }
)) {
    console.log(`node: ${metadata.langgraph_node}`);
    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
11 of 18 1/20/2026, 3:00 PM
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);
const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});
for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "custom" }
)) {
    console.log(chunk);
}
Looking up data for city: San Francisco
Acquired data for city: San Francisco
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
12 of 18 1/20/2026, 3:00 PM
If you add the  parameter to your tool, you wonʼt be able to invoke the tool outside
of a LangGraph execution context without providing a writer function.
You can specify multiple streaming modes by passing streamMode as an array:
.
The streamed outputs will be tuples of  where  is the name of the
stream mode and  is the data streamed by that mode.
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
13 of 18 1/20/2026, 3:00 PM
In some applications you might need to disable streaming of individual tokens for a given
model. This is useful when:
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);
const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});
for await (const [streamMode, chunk] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: ["updates", "messages", "custom"] }
)) {
    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
14 of 18 1/20/2026, 3:00 PM
Set  when initializing the model.
When deploying to LangSmith, set  on any models whose output you
donʼt want streamed to the client. This is configured in your graph code before deployment.
Not all chat model integrations support the  parameter. If your model doesnʼt
support it, use  instead. This parameter is available on all chat
models via the base class.
See the  for more details.
Working with  systems to control which agents stream their output
Mixing models that support streaming with those that do not
Deploying to  and wanting to prevent certain model outputs from being
streamed to the client
 — Build React UIs with  for real-time agent
interactions
 — Stream tokens directly from a chat model without
using an agent or graph
 — Stream agent progress while handling
interrupts for human review
 — Advanced streaming options including , 
import { ChatOpenAI } from "@langchain/openai";
const model = new ChatOpenAI({
  model: "gpt-4o",
});
  streaming: false,  
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
15 of 18 1/20/2026, 3:00 PM
Was this page helpful? Yes No
 or .
 to Claude, VSCode, and more via MCP for real-time answers.
modes, and subgraph streaming
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
16 of 18 1/20/2026, 3:00 PM
Forum
Changelog
LangChain Academy
Trust Center
About
Careers
Blog
Powered by
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
17 of 18 1/20/2026, 3:00 PM
Streaming
Overview - Docs by LangChain https://docs.langchain.com/oss/javascript/langchain/streaming/overview
18 of 18 1/20/2026, 3:00 PM